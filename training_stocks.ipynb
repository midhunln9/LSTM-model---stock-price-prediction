{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7tyIE9B7Ie-",
        "outputId": "25c02952-4c83-4d82-b076-e3f4f298eb96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vslLGmw87KzU",
        "outputId": "dd4e815c-811d-4445-c5a2-508cd09ee99b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.42)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.1.4)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2024.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.6)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install yfinance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU_lLCI1a-3"
      },
      "source": [
        "# Give all the stock symbols as a List ( Information Technology stock )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "S9Syh7RX1b25",
        "outputId": "50652d14-2322-4cef-8301-a88bfd76eb0d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAAPL - Apple Inc.\\nMSFT - Microsoft Corporation\\nNVDA - NVIDIA Corporation\\nADBE - Adobe Inc.\\nCRM - Salesforce, Inc.\\nORCL - Oracle Corporation\\nCSCO - Cisco Systems, Inc.\\nINTC - Intel Corporation\\nIBM - International Business Machines Corporation\\nQCOM - Qualcomm Incorporated\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stock_symbols = [\"AAPL\", \"MSFT\", \"NVDA\", \"ADBE\", \"CRM\", \"ORCL\", \"CSCO\", \"INTC\", \"IBM\", \"QCOM\"]\n",
        "\n",
        "\"\"\"\n",
        "AAPL - Apple Inc.\n",
        "MSFT - Microsoft Corporation\n",
        "NVDA - NVIDIA Corporation\n",
        "ADBE - Adobe Inc.\n",
        "CRM - Salesforce, Inc.\n",
        "ORCL - Oracle Corporation\n",
        "CSCO - Cisco Systems, Inc.\n",
        "INTC - Intel Corporation\n",
        "IBM - International Business Machines Corporation\n",
        "QCOM - Qualcomm Incorporated\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJs5NsSp1OpD"
      },
      "source": [
        "# Import all libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaViG0rb0MId"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense, Input, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad, Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import GlorotUniform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exuAEvLb1XD3"
      },
      "source": [
        "# Step 1 : Download stock data and store in \".csv\" format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpGLkg8B1skG"
      },
      "outputs": [],
      "source": [
        "def download_stock_data(symbols, start_date, end_date, directory):\n",
        "    \"\"\"\n",
        "    Downloads stock data for given symbols and stores them as CSV files.\n",
        "\n",
        "    Args:\n",
        "        symbols (list): List of stock symbols to download.\n",
        "        start_date (str): Start date for data download in 'YYYY-MM-DD' format.\n",
        "        end_date (str): End date for data download in 'YYYY-MM-DD' format.\n",
        "        directory (str): Directory to save the CSV files.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    for symbol in symbols:\n",
        "        data = yf.download(symbol, start=start_date, end=end_date)\n",
        "        file_path = os.path.join(directory, f\"{symbol}.csv\")\n",
        "        data.to_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKHJpYci1wF-"
      },
      "source": [
        "# Step 2 : Read the stock data from the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G1hfTnX1wS-"
      },
      "outputs": [],
      "source": [
        "def read_stock_data(directory, symbols):\n",
        "    \"\"\"\n",
        "    Reads stock data from CSV files and returns it as a dictionary of DataFrames.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Directory where the CSV files are stored.\n",
        "        symbols (list): List of stock symbols to read.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with stock symbols as keys and corresponding DataFrames as values.\n",
        "    \"\"\"\n",
        "    stock_data = {}\n",
        "    for symbol in symbols:\n",
        "        file_path = os.path.join(directory, f\"{symbol}.csv\")\n",
        "        stock_data[symbol] = pd.read_csv(file_path)\n",
        "    return stock_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVAw_crm13BX"
      },
      "source": [
        "# Step 3 : Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7cJOYU113nn"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data, window_size):\n",
        "    \"\"\"\n",
        "    Preprocesses the stock data for training.\n",
        "\n",
        "    Args:\n",
        "        data (DataFrame): Stock data containing a 'Close' column.\n",
        "        window_size (int): Size of the window to create sequences.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the processed input data, target data, and the scaler used.\n",
        "    \"\"\"\n",
        "    data = data[['Close']]\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(scaled_data)):\n",
        "        X.append(scaled_data[i - window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    return X, y, scaler\n",
        "\n",
        "def split_data(X, y, train_size=0.7, val_size=0.15):\n",
        "    \"\"\"\n",
        "    Splits the data into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        X (ndarray): Input data.\n",
        "        y (ndarray): Target data.\n",
        "        train_size (float): Proportion of data to be used for training.\n",
        "        val_size (float): Proportion of data to be used for validation.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train_index = int(len(X) * train_size)\n",
        "    val_index = train_index + int(len(X) * val_size)\n",
        "\n",
        "    X_train, y_train = X[:train_index], y[:train_index]\n",
        "    X_val, y_val = X[train_index:val_index], y[train_index:val_index]\n",
        "    X_test, y_test = X[val_index:], y[val_index:]\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywWYiSpS2AMS"
      },
      "source": [
        "# Step 4 : Build LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v451CtcR2Alk"
      },
      "outputs": [],
      "source": [
        "def build_model(sequence_length):\n",
        "    \"\"\"\n",
        "    Builds a Sequential model with LSTM and Dense layers.\n",
        "\n",
        "    Args:\n",
        "        sequence_length (int): Length of the input sequences.\n",
        "\n",
        "    Returns:\n",
        "        model: Compiled Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(sequence_length, 1)))\n",
        "\n",
        "    # LSTM layers\n",
        "    model.add(Bidirectional(LSTM(units=150, return_sequences=True,\n",
        "                                 kernel_initializer=GlorotUniform(),\n",
        "                                 recurrent_initializer=GlorotUniform(),\n",
        "                                 activation='tanh')))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Bidirectional(LSTM(units=150, return_sequences=True,\n",
        "                                 kernel_initializer=GlorotUniform(),\n",
        "                                 recurrent_initializer=GlorotUniform(),\n",
        "                                 activation='tanh')))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Bidirectional(LSTM(units=150,\n",
        "                                 kernel_initializer=GlorotUniform(),\n",
        "                                 recurrent_initializer=GlorotUniform(),\n",
        "                                 activation='tanh')))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Dense(units=64, activation='relu', kernel_initializer=GlorotUniform()))\n",
        "    model.add(Dense(units=32, activation='relu', kernel_initializer=GlorotUniform()))\n",
        "    model.add(Dense(units=1, activation='linear', kernel_initializer=GlorotUniform()))\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96zO0VKF2I7U"
      },
      "source": [
        "# Step 5 : Perform hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_X0hGpn2LZ5"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_tuning(X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "                          sequence_length, results_dir, symbol):\n",
        "    \"\"\"\n",
        "    Tunes the model hyperparameters using different optimizers, learning rates,\n",
        "    and batch sizes. Records the results for each combination.\n",
        "\n",
        "    Args:\n",
        "        X_train (ndarray): Training input data.\n",
        "        y_train (ndarray): Training target data.\n",
        "        X_val (ndarray): Validation input data.\n",
        "        y_val (ndarray): Validation target data.\n",
        "        X_test (ndarray): Test input data.\n",
        "        y_test (ndarray): Test target data.\n",
        "        sequence_length (int): Length of the input sequences.\n",
        "        results_dir (str): Directory to save the hyperparameter tuning results.\n",
        "        symbol (str): Stock symbol being processed.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing RMSE values for each combination of hyperparameters.\n",
        "    \"\"\"\n",
        "    optimizers = [Adam, Adagrad, Nadam]\n",
        "    learning_rates = [0.01, 0.001, 0.0001]\n",
        "    batch_sizes = [50, 100, 150]\n",
        "    n_replicates = 3\n",
        "\n",
        "    # Define the EarlyStopping callback\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',            # Monitor the validation loss\n",
        "        patience=10,                   # Stop after 10 epochs with no improvement\n",
        "        verbose=1,                     # Verbose output when stopping early\n",
        "        restore_best_weights=True      # Restore the model weights from the epoch with the best validation loss\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Iterate through all combinations of optimizers, learning rates, and batch sizes\n",
        "    for optimizer in optimizers:\n",
        "        for lr in learning_rates:\n",
        "            for batch_size in batch_sizes:\n",
        "                rmse_list = []\n",
        "                for _ in range(n_replicates):\n",
        "                    model = build_model(sequence_length)\n",
        "                    optimizer_instance = optimizer(learning_rate=lr)\n",
        "                    model.compile(optimizer=optimizer_instance, loss='mean_squared_error')\n",
        "                    model.fit(X_train, y_train, epochs=200, batch_size=batch_size,\n",
        "                              validation_data=(X_val, y_val),\n",
        "                              callbacks=[early_stopping], verbose=0)\n",
        "                    y_pred = model.predict(X_test)\n",
        "                    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "                    rmse_list.append(rmse)\n",
        "\n",
        "                avg_rmse = np.mean(rmse_list)\n",
        "                key = f\"Optimizer:{optimizer.__name__}_LR:{lr}_BatchSize:{batch_size}\"\n",
        "                results[key] = avg_rmse\n",
        "\n",
        "    json_file = os.path.join(results_dir, f'{symbol}_results.json')\n",
        "    with open(json_file, 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOEa6r722N1k"
      },
      "source": [
        "# Step 6 : Extract the best hyperparameters from the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSmHsBuI2SBP"
      },
      "outputs": [],
      "source": [
        "def get_best_hyperparameters(results):\n",
        "    \"\"\"\n",
        "    Extracts the best hyperparameters based on the lowest RMSE value.\n",
        "\n",
        "    Args:\n",
        "        results (dict): Dictionary containing RMSE values for each combination of hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Best optimizer, learning rate, and batch size.\n",
        "    \"\"\"\n",
        "    best_result = min(results, key=results.get)\n",
        "    best_optimizer_name = best_result.split('_')[0].split(':')[1]\n",
        "    best_lr = float(best_result.split('_')[1].split(':')[1])\n",
        "    best_batch_size = int(best_result.split('_')[2].split(':')[1])\n",
        "\n",
        "    # Select the appropriate optimizer\n",
        "    if best_optimizer_name == 'Adam':\n",
        "        best_optimizer = Adam(learning_rate=best_lr)\n",
        "    elif best_optimizer_name == 'Adagrad':\n",
        "        best_optimizer = Adagrad(learning_rate=best_lr)\n",
        "    elif best_optimizer_name == 'Nadam':\n",
        "        best_optimizer = Nadam(learning_rate=best_lr)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {best_optimizer_name}\")\n",
        "\n",
        "    return best_optimizer, best_batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uHnxEn72V4f"
      },
      "source": [
        "# Step 7 : Train the model with best Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfMzrIrW2cIP"
      },
      "outputs": [],
      "source": [
        "def train_best_model(X_train, y_train, X_val, y_val, sequence_length, best_optimizer, best_batch_size):\n",
        "    \"\"\"\n",
        "    Trains the model using the best hyperparameters obtained from tuning.\n",
        "\n",
        "    Args:\n",
        "        X_train (ndarray): Training input data.\n",
        "        y_train (ndarray): Training target data.\n",
        "        X_val (ndarray): Validation input data.\n",
        "        y_val (ndarray): Validation target data.\n",
        "        sequence_length (int): Length of the input sequences.\n",
        "        best_optimizer (Optimizer): Best optimizer selected from tuning.\n",
        "        best_batch_size (int): Best batch size selected from tuning.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Trained model and training history.\n",
        "    \"\"\"\n",
        "    model = build_model(sequence_length)\n",
        "    model.compile(optimizer=best_optimizer, loss='mean_squared_error')\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=200, batch_size=best_batch_size,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6-Tgu6b2d-s"
      },
      "source": [
        "# Step 8 : Plot the Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daPJnwxD2iya"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(history):\n",
        "    \"\"\"\n",
        "    Plots the learning curves for training and validation loss.\n",
        "\n",
        "    Args:\n",
        "        history (History): Training history obtained from model fitting.\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIPM0nNG2j6W"
      },
      "source": [
        "# Step 9 : Evaluation of Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxAOohNi2pb_"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, scaler, metrics_dir, symbol):\n",
        "    \"\"\"\n",
        "    Evaluates the model on test data and saves the evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        model (Sequential): Trained model.\n",
        "        X_test (ndarray): Test input data.\n",
        "        y_test (ndarray): Test target data.\n",
        "        scaler (MinMaxScaler): Scaler used during preprocessing to rescale data.\n",
        "        metrics_dir (str): Directory to save the evaluation metrics.\n",
        "        symbol (str): Stock symbol being processed.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing evaluation metrics such as MSE, RMSE, MAE, and MAPE.\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
        "    rmse = sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
        "    mape = np.mean(np.abs((y_test_rescaled - y_pred_rescaled) / y_test_rescaled)) * 100\n",
        "\n",
        "    metrics = {\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"MAPE\": mape\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(metrics_dir):\n",
        "        os.makedirs(metrics_dir)\n",
        "\n",
        "    json_file = os.path.join(metrics_dir, f'{symbol}_evaluation_metrics.json')\n",
        "    with open(json_file, 'w') as f:\n",
        "        json.dump(metrics, f, indent=4)\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJZsgCN2qX6"
      },
      "source": [
        "# Step 10 : Retrain the model on combined dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-sX4q0s2v6S"
      },
      "outputs": [],
      "source": [
        "def retrain_on_full_data(data, sequence_length, best_optimizer, best_batch_size):\n",
        "    \"\"\"\n",
        "    Retrains the model on the full dataset using the best hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        data (DataFrame): Full stock data.\n",
        "        sequence_length (int): Length of the input sequences.\n",
        "        best_optimizer (Optimizer): Best optimizer selected from tuning.\n",
        "        best_batch_size (int): Best batch size selected from tuning.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Retrained model and training history.\n",
        "    \"\"\"\n",
        "    X, y, _ = preprocess_data(data, sequence_length)\n",
        "    model = build_model(sequence_length)\n",
        "    model.compile(optimizer=best_optimizer, loss='mean_squared_error')\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(X, y, epochs=200, batch_size=best_batch_size, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVxtq6i32wcZ"
      },
      "source": [
        "# Step 11 : Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VkBno6B20q_"
      },
      "outputs": [],
      "source": [
        "def save_trained_model(model, model_dir, symbol):\n",
        "    \"\"\"\n",
        "    Saves the trained model to a specified directory.\n",
        "\n",
        "    Args:\n",
        "        model (Sequential): Trained model to be saved.\n",
        "        model_dir (str): Directory where the model should be saved.\n",
        "        symbol (str): Stock symbol to use in the filename.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model.save(os.path.join(model_dir, f\"{symbol}_model.h5\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJOHI_-421S2"
      },
      "source": [
        "# Step 12 : Main Execution flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "k25MpdmQ26wP",
        "outputId": "bd483c82-918b-442a-8afd-3eebc8d6a09c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing stock: AAPL\n",
            "Epoch 27: early stopping\n",
            "Restoring model weights from the end of the best epoch: 17.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 253ms/step\n",
            "Epoch 43: early stopping\n",
            "Restoring model weights from the end of the best epoch: 33.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 263ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 265ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 253ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 271ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 252ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 269ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 365ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 265ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 269ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 404ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 383ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 285ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 402ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 282ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 354ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 289ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 379ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 412ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 263ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 311ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 283ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 252ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 254ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 295ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 315ms/step\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 340ms/step\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"stock_data\"              # Directory to store stock data\n",
        "results_dir = \"hyperparameter_results\"  # Directory to store hyperparameter tuning results\n",
        "metrics_dir = \"evaluation_metrics\"   # Directory to store evaluation metrics\n",
        "model_dir = \"trained_models\"         # Directory to save trained models\n",
        "start_date = '2010-08-01'            # Start date for stock data download\n",
        "end_date = '2023-12-31'              # End date for stock data download\n",
        "sequence_length = 60                 # Length of the input sequences\n",
        "\n",
        "# Download and preprocess stock data\n",
        "download_stock_data(stock_symbols, start_date, end_date, data_dir)\n",
        "stock_data = read_stock_data(data_dir, stock_symbols)\n",
        "\n",
        "# Process each stock symbol\n",
        "for symbol in stock_symbols:\n",
        "    print(f\"Processing stock: {symbol}\")\n",
        "    data = stock_data[symbol]\n",
        "    X, y, scaler = preprocess_data(data, sequence_length)\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = split_data(X, y)\n",
        "\n",
        "    # Perform hyperparameter tuning\n",
        "    results = hyperparameter_tuning(X_train, y_train, X_val, y_val, X_test, y_test, sequence_length, results_dir, symbol)\n",
        "\n",
        "    # Get the best hyperparameters and train the model\n",
        "    best_optimizer, best_batch_size = get_best_hyperparameters(results)\n",
        "    model, history = train_best_model(X_train, y_train, X_val, y_val, sequence_length, best_optimizer, best_batch_size)\n",
        "\n",
        "    # Plot the learning curve and evaluate the model\n",
        "    plot_learning_curve(history)\n",
        "    evaluate_model(model, X_test, y_test, scaler, metrics_dir, symbol)\n",
        "\n",
        "    # Retrain the model on the full dataset and save it\n",
        "    best_optimizer, best_batch_size = get_best_hyperparameters(results)\n",
        "    final_model, final_history = retrain_on_full_data(data, sequence_length, best_optimizer, best_batch_size)\n",
        "    save_trained_model(final_model, model_dir, symbol)\n",
        "\n",
        "    print(f\"Completed processing for stock: {symbol}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
